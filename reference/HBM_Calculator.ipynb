{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TofAGpcW05p"
      },
      "source": [
        "# ⚙️ How Much VRAM Does My Model Need?\n",
        "\n",
        "This notebook estimates the **total VRAM required** to run inference for your specific use case. To get a tailored calculation, you'll specify:\n",
        "\n",
        "* A model from the Hugging Face Hub.\n",
        "* Your Hugging Face API key (for gated models).\n",
        "* The number of model parameters (in billions).\n",
        "* The average input and output length (in tokens).\n",
        "* The **max number of sequences** you plan to use for inference.\n",
        "  * NOTE that a max_num_sequences of one will provide you with the minimum required xPU memory required for your workload.\n",
        "\n",
        "The final calculation will help you select the right xPU hardware. For context, here are the memory capacities of some common accelerator types:\n",
        "\n",
        "```text\n",
        "1 L4 GPU          = 24 GB\n",
        "1 H100 GPU        = 80 GB\n",
        "1-chip v5e TPU    = 16 GB\n",
        "8-chip v5e TPU    = 128 GB\n",
        "8x L4 GPUs        = 192 GB\n",
        "8x H100 GPUs      = 640 GB\n",
        "```\n",
        "\n",
        "---  \n",
        "\n",
        "\n",
        "## What This Notebook Calculates\n",
        "\n",
        "The notebook breaks down the VRAM requirement into several key components before providing a final, actionable number:\n",
        "\n",
        "* **Model Weight Memory**: The memory needed to load the model's parameters onto the accelerator.\n",
        "* **Activation Memory**: The temporary \"scratchpad\" memory used during computation. This scales directly with your max_num_sequences and sequence length.\n",
        "* **KV Cache Memory**: The memory required to store the attention context (Key-Value cache) for the entire max_num_sequences, which is crucial for efficient token generation.\n",
        "* **Total Required Memory**: The final estimated VRAM your configuration will need. This is the sum of the model weight, system overhead, activation memory, and KV cache for your specified max_num_sequences.\n",
        "\n",
        "*Note: Multi-xPU instances might incur additional memory usage for communication overhead, depending on how the model and data are distributed across devices.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkChG5g9R4CR",
        "outputId": "506e89f3-36f0-4843-b2b9-547fdfa6d1d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Fetching configuration for google/gemma-3-27b-it ---\n",
            "Using nested 'text_config' for model parameters.\n",
            "\n",
            "--- Calculating Required GPU/TPU Memory ---\n",
            "1. Model Weight Memory: 54.00 GB\n",
            "2. System Overhead Memory: 1.00 GB\n",
            "3. PyTorch Activation Memory (for max_num_sequences 1): 0.31 GB\n",
            "4. KV Cache Memory (for max_num_sequences 1): 1.13 GB\n",
            "\n",
            "--- Total Memory Calculation ---\n",
            "\n",
            "-------------------------------------\n",
            "✅ Required GPU/TPU Memory: 56.44 GB\n",
            "-------------------------------------\n",
            "\n",
            "This is the estimated total GPU/TPU VRAM needed to run inference for max_num_sequences of 1 with the specified model and sequence lengths.\n"
          ]
        }
      ],
      "source": [
        "# @title Resource requirements for LLM inference\n",
        "huggingface_api_key = \"\" # @param {type:\"string\"}\n",
        "huggingface_model_name = \"google/gemma-3-27b-it\" # @param {type:\"string\"}\n",
        "model_parameters_in_billions = 27 # @param {type:\"number\"}\n",
        "avg_input_length = 1500 # @param {type:\"integer\"}\n",
        "avg_output_length = 200 # @param {type:\"integer\"}\n",
        "max_num_sequences = 1 # @param {type:\"integer\"}\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# --- Step 1: Fetch Model Configuration ---\n",
        "print(f\"--- Fetching configuration for {huggingface_model_name} ---\")\n",
        "\n",
        "# Create a directory to store the config files\n",
        "os.makedirs(\"config_files\", exist_ok=True)\n",
        "config_path = os.path.join(\"config_files\", \"config.json\")\n",
        "\n",
        "# Download the model's config.json file\n",
        "config_url = f\"https://huggingface.co/{huggingface_model_name}/resolve/main/config.json?download=true\"\n",
        "headers = {\"Authorization\": f\"Bearer {huggingface_api_key}\"}\n",
        "response = requests.get(config_url, headers=headers)\n",
        "response.raise_for_status()\n",
        "\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "with open(config_path, \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "# Check for a nested text_config, common in multimodal models.\n",
        "if \"text_config\" in data and isinstance(data[\"text_config\"], dict):\n",
        "    config_source = data[\"text_config\"]\n",
        "    print(\"Using nested 'text_config' for model parameters.\")\n",
        "else:\n",
        "    config_source = data\n",
        "    print(\"Using top-level config for model parameters.\")\n",
        "\n",
        "# Use .get() for safe dictionary access to extract model architecture details\n",
        "hidden_size = config_source.get('hidden_size')\n",
        "num_hidden_layers = config_source.get('num_hidden_layers')\n",
        "num_attention_heads = config_source.get('num_attention_heads')\n",
        "intermediate_size = config_source.get('intermediate_size')\n",
        "num_kv_heads = config_source.get('num_key_value_heads', num_attention_heads)\n",
        "\n",
        "# Ensure all required parameters were found\n",
        "required_params = [hidden_size, num_hidden_layers, num_attention_heads, intermediate_size]\n",
        "if not all(required_params):\n",
        "    raise ValueError(\"One or more required model parameters (e.g., hidden_size, num_hidden_layers) could not be found in the config.\")\n",
        "\n",
        "head_dims = hidden_size // num_attention_heads\n",
        "dtype = data.get('torch_dtype', 'bfloat16')\n",
        "\n",
        "# Determine data type size in bytes based on model's precision (dtype)\n",
        "match dtype:\n",
        "  case 'float16' | 'bfloat16':\n",
        "    parameter_data_type_size = 2\n",
        "    kv_data_type_size = 2\n",
        "  case 'float32':\n",
        "    parameter_data_type_size = 4\n",
        "    kv_data_type_size = 4\n",
        "  case _: # Default to bfloat16 if not specified\n",
        "    parameter_data_type_size = 2\n",
        "    kv_data_type_size = 2\n",
        "\n",
        "# --- Step 2: Calculate Required xPU Memory ---\n",
        "print(\"\\n--- Calculating Required xPU Memory ---\")\n",
        "\n",
        "# Component 1: Model Weight Memory (Static)\n",
        "# Memory needed to load the model's parameters onto the xPU.\n",
        "number_of_model_parameters = model_parameters_in_billions * 1e9\n",
        "model_weight_bytes = number_of_model_parameters * parameter_data_type_size\n",
        "model_weight_gb = model_weight_bytes / (1000**3)\n",
        "print(f\"1. Model Weight Memory: {model_weight_gb:.2f} GB\")\n",
        "\n",
        "# Component 2: Overhead Memory (Static)\n",
        "# Fixed memory for non-PyTorch components like CUDA kernels, etc.\n",
        "overhead_memory_gb = 1.0\n",
        "print(f\"2. System Overhead Memory: {overhead_memory_gb:.2f} GB\")\n",
        "\n",
        "# Component 3: PyTorch Activation Peak Memory (Dynamic)\n",
        "# Memory for storing intermediate calculations (activations) during the forward pass.\n",
        "# This scales with max_num_sequences and sequence length.\n",
        "sequence_length = avg_input_length + avg_output_length\n",
        "pytorch_activation_peak_memory_bytes = max_num_sequences * sequence_length * (18 * hidden_size + 4 * intermediate_size)\n",
        "pytorch_activation_peak_memory_gb = pytorch_activation_peak_memory_bytes / (1000**3)\n",
        "print(f\"3. PyTorch Activation Memory (for max_num_sequences size {max_num_sequences}): {pytorch_activation_peak_memory_gb:.2f} GB\")\n",
        "\n",
        "# Component 4: KV Cache Memory (Dynamic)\n",
        "# Memory for the Key-Value cache, which stores attention context to speed up token generation.\n",
        "# This scales with max_num_sequences and sequence length.\n",
        "kv_vectors = 2 # One for Key, one for Value\n",
        "kv_cache_memory_per_batch_bytes = kv_vectors * max_num_sequences * sequence_length * num_kv_heads * head_dims * num_hidden_layers * kv_data_type_size\n",
        "kv_cache_memory_per_batch_gb = kv_cache_memory_per_batch_bytes / (1000**3)\n",
        "print(f\"4. KV Cache Memory (for max_num_sequences {max_num_sequences}): {kv_cache_memory_per_batch_gb:.2f} GB\")\n",
        "\n",
        "# --- Final Calculation ---\n",
        "print(\"\\n--- Total Memory Calculation ---\")\n",
        "# Sum of static and dynamic memory components.\n",
        "required_gpu_memory_gb = (\n",
        "    model_weight_gb +\n",
        "    overhead_memory_gb +\n",
        "    pytorch_activation_peak_memory_gb +\n",
        "    kv_cache_memory_per_batch_gb\n",
        ")\n",
        "\n",
        "print(\"\\n-------------------------------------\")\n",
        "print(f\"✅ Required xPU Memory: {required_gpu_memory_gb:.2f} GB\")\n",
        "print(\"-------------------------------------\")\n",
        "print(f\"\\nThis is the estimated total xPU VRAM needed to run inference for a max_num_sequences of {max_num_sequences} with the specified model and sequence lengths.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYZbPvbD6vvs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}